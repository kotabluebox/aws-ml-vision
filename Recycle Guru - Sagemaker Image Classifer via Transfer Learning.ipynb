{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recycle Guru - Training and Deploying a Custom Image Classifier with Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure where to fetch our training data\n",
    "\n",
    "All of our images live inside an S3 bucket, organized into folders in a structure similar to this:\n",
    "\n",
    "```\n",
    "recycle-images\n",
    "├── coffee\n",
    "│   ├── cup1.jpg\n",
    "│   ├── cup2.jpg\n",
    "|   ├── cup2.jpg\n",
    "│   └── . . .\n",
    "└── water_bottle\n",
    "│   ├── waterbottle1.jpg\n",
    "│   ├── waterbottle2.jpg    \n",
    "│   ├── waterbottle3.jpg\n",
    "│   ├── . . .\n",
    "└── . . .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An S3 Bucket Name\n",
    "data_bucket_name='recycle-guru'\n",
    "\n",
    "# A prefix name inside the S3 bucket containing sub-folders of images (one per label class)\n",
    "dataset_name = 'recycle-images' \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n",
    "Here we set up the linkage and authentication to AWS services\n",
    "\n",
    "- The role used to give learning and hosting access to your data. This will automatically be obtained from the role used to start the notebook\n",
    "- A `session` variable that holds some configuration state for interacting with SageMaker from Python and contains some methods for preparing input data\n",
    "- A reference to the Amazon sagemaker image classification docker image \n",
    "\n",
    "More info about the SageMaker built-in Image Classification algorithm here: https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "training_image = get_image_uri(sess.boto_region_name, 'image-classification', repo_version=\"latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for our model\n",
    "Before we can train our model, we need to:\n",
    "\n",
    "- Create some files that will teach SageMaker about the images in each of our classes\n",
    "- Upload these additional files to S3\n",
    "- Configure our model to use these files for training and validating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the im2rec.py script on this system\n",
    "The SageMaker image classifier algorithm needs to know about which images belong to which classes. We provide this data using either LST or RecordIO files. We'll use a python script called `im2rec.py` to create these files.\n",
    "\n",
    "More info here: https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html#IC-inputoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: BASE_DIR=/tmp\n",
      "env: S3_DATA_BUCKET_NAME=recycle-guru\n",
      "env: DATASET_NAME=recycle-images\n",
      "env: IM2REC=/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/tools/im2rec.py\n"
     ]
    }
   ],
   "source": [
    "# Find im2rec in our environment and set up some other vars in our environemnt\n",
    "\n",
    "base_dir='/tmp'\n",
    "\n",
    "%env BASE_DIR=$base_dir\n",
    "%env S3_DATA_BUCKET_NAME = $data_bucket_name\n",
    "%env DATASET_NAME = $dataset_name\n",
    "\n",
    "import sys,os\n",
    "\n",
    "suffix='/mxnet/tools/im2rec.py'\n",
    "im2rec = list(filter( (lambda x: os.path.isfile(x + suffix )), sys.path))[0] + suffix\n",
    "%env IM2REC=$im2rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get our training images from S3\n",
    "In order to create training and validation RecordIO files, we need to download our images to our local filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull our images from S3\n",
    "!aws s3 sync s3://$S3_DATA_BUCKET_NAME/$DATASET_NAME $BASE_DIR/$DATASET_NAME --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create RecordIO files from our training images\n",
    "The `im2rec.py` script can create LST files and/or RecordIO files from our training data. \n",
    "\n",
    "More info here: https://mxnet.incubator.apache.org/versions/master/faq/recordio.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating LST files\n",
      "Label classes:\n",
      "coffee 0\n",
      "coffee_cup 1\n",
      "markers 2\n",
      "raspberry_pi 3\n",
      "timmy 4\n",
      "water_bottle 5\n",
      "Creating RecordIO files\n",
      "Creating .rec file from /tmp/recycle-images_train.lst in /tmp\n",
      "time: 0.07599377632141113  count: 0\n",
      "Creating .rec file from /tmp/recycle-images_test.lst in /tmp\n",
      "time: 0.06701040267944336  count: 0\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 1.6M Oct 30 20:22 recycle-images_test.rec\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 3.5M Oct 30 20:22 recycle-images_train.rec\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Use the IM2REC script to convert our images into RecordIO files\n",
    "\n",
    "# Clean up our working dir of existing LST and REC files\n",
    "cd $BASE_DIR\n",
    "rm *.rec\n",
    "rm *.lst\n",
    "\n",
    "# First we need to create two LST files (training and test lists), noting the correct label class for each image\n",
    "# We'll also save the output of the LST files command, since it includes a list of all of our label classes\n",
    "echo \"Creating LST files\"\n",
    "python $IM2REC --list --recursive --pass-through --test-ratio=0.3 --train-ratio=0.7 $DATASET_NAME $DATASET_NAME > ${DATASET_NAME}_classes\n",
    "\n",
    "echo \"Label classes:\"\n",
    "cat ${DATASET_NAME}_classes\n",
    "\n",
    "# Then we create RecordIO files from the LST files\n",
    "echo \"Creating RecordIO files\"\n",
    "python $IM2REC --num-thread=4 ${DATASET_NAME}_train.lst $DATASET_NAME\n",
    "python $IM2REC --num-thread=4 ${DATASET_NAME}_test.lst $DATASET_NAME\n",
    "ls -lh *.rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload our training and test data RecordIO files so we can train with them\n",
    "Now that we have our training and test .rec files, we upload them to S3 so SageMaker can use them for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://sagemaker-us-east-1-023375022819/recycle-images/train/recycle-images_train.rec\n",
      "delete: s3://sagemaker-us-east-1-023375022819/recycle-images/validation/recycle-images_test.rec\n",
      "upload: ../../../../tmp/recycle-images_train.rec to s3://sagemaker-us-east-1-023375022819/recycle-images/train/recycle-images_train.rec\n",
      "upload: ../../../../tmp/recycle-images_test.rec to s3://sagemaker-us-east-1-023375022819/recycle-images/validation/recycle-images_test.rec\n"
     ]
    }
   ],
   "source": [
    "# Upload our train and test RecordIO files to S3 in the bucket that our sagemaker session is using\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "s3train_path = 's3://{}/{}/train/'.format(bucket, dataset_name)\n",
    "s3validation_path = 's3://{}/{}/validation/'.format(bucket, dataset_name)\n",
    "\n",
    "# Clean up any existing data\n",
    "!aws s3 rm s3://{bucket}/{dataset_name}/train --recursive\n",
    "!aws s3 rm s3://{bucket}/{dataset_name}/validation --recursive\n",
    "\n",
    "# Upload the rec files to the train and validation channels\n",
    "!aws s3 cp /tmp/{dataset_name}_train.rec $s3train_path\n",
    "!aws s3 cp /tmp/{dataset_name}_test.rec $s3validation_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the data for our model training to use\n",
    "Finally, we tell SageMaker where to find these RecordIO files to use for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.session.s3_input(\n",
    "    s3train_path, \n",
    "    distribution='FullyReplicated', \n",
    "    content_type='application/x-recordio', \n",
    "    s3_data_type='S3Prefix'\n",
    ")\n",
    "\n",
    "validation_data = sagemaker.session.s3_input(\n",
    "    s3validation_path, \n",
    "    distribution='FullyReplicated', \n",
    "    content_type='application/x-recordio', \n",
    "    s3_data_type='S3Prefix'\n",
    ")\n",
    "\n",
    "data_channels = {'train': train_data, 'validation': validation_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now it's time to train our model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an image classifier object with some base configuration\n",
    "More info here: https://sagemaker.readthedocs.io/en/stable/estimators.html#sagemaker.estimator.Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, dataset_name)\n",
    "\n",
    "image_classifier = sagemaker.estimator.Estimator(\n",
    "    training_image,\n",
    "    role, \n",
    "    train_instance_count=1, \n",
    "    train_instance_type='ml.p3.2xlarge',\n",
    "    output_path=s3_output_location,\n",
    "    sagemaker_session=sess\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set some training hyperparameters\n",
    "\n",
    "Finally, before we train, we provide some additional configuration parameters for the training.\n",
    "\n",
    "More info here: https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'use_pretrained_model': 1,\n",
       " 'image_shape': '3,224,224',\n",
       " 'num_classes': 6,\n",
       " 'num_training_samples': 211,\n",
       " 'augmentation_type': 'crop_color_transform',\n",
       " 'learning_rate': 0.001,\n",
       " 'mini_batch_size': 5}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes=! ls -l {base_dir}/{dataset_name} | wc -l\n",
    "num_classes=int(num_classes[0]) - 1\n",
    "\n",
    "num_training_samples=! cat {base_dir}/{dataset_name}_train.lst | wc -l\n",
    "num_training_samples = int(num_training_samples[0])\n",
    "\n",
    "# Learn more about the Sagemaker built-in Image Classifier hyperparameters here: https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html\n",
    "\n",
    "# These hyperparameters we won't want to change, as they define things like\n",
    "# the size of the images we'll be sending for input, the number of training classes we have, etc.\n",
    "base_hyperparameters=dict(\n",
    "    use_pretrained_model=1,\n",
    "    image_shape='3,224,224',\n",
    "    num_classes=num_classes,\n",
    "    num_training_samples=num_training_samples,\n",
    "    augmentation_type = 'crop_color_transform'\n",
    ")\n",
    "\n",
    "# These are hyperparameters we may want to tune, as they can affect the model training success:\n",
    "hyperparameters={\n",
    "    **base_hyperparameters, \n",
    "    **dict(\n",
    "        learning_rate=0.001,\n",
    "        mini_batch_size=5,\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "image_classifier.set_hyperparameters(**hyperparameters)\n",
    "\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the training\n",
    "Train our model!\n",
    "\n",
    "This will take some time because it's provisioning a new container runtime to train our model, then the actual training happens, then the trained model gets uploaded to S3 and the container is shut down.\n",
    "\n",
    "More info here: https://sagemaker.readthedocs.io/en/stable/estimators.html#sagemaker.estimator.Estimator.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-30 20:22:50 Starting - Starting the training job...\n",
      "2019-10-30 20:22:51 Starting - Launching requested ML instances......\n",
      "2019-10-30 20:23:58 Starting - Preparing the instances for training......\n",
      "2019-10-30 20:25:10 Downloading - Downloading input data\n",
      "2019-10-30 20:25:10 Training - Downloading the training image.....\u001b[31mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:06 INFO 139813596038976] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/image_classification/default-input.json: {u'beta_1': 0.9, u'gamma': 0.9, u'beta_2': 0.999, u'optimizer': u'sgd', u'use_pretrained_model': 0, u'eps': 1e-08, u'epochs': 30, u'lr_scheduler_factor': 0.1, u'num_layers': 152, u'image_shape': u'3,224,224', u'precision_dtype': u'float32', u'mini_batch_size': 32, u'weight_decay': 0.0001, u'learning_rate': 0.1, u'momentum': 0}\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:06 INFO 139813596038976] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'image_shape': u'3,224,224', u'learning_rate': u'0.001', u'augmentation_type': u'crop_color_transform', u'mini_batch_size': u'5', u'use_pretrained_model': u'1', u'num_classes': u'6', u'num_training_samples': u'211'}\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:06 INFO 139813596038976] Final configuration: {u'optimizer': u'sgd', u'learning_rate': u'0.001', u'epochs': 30, u'lr_scheduler_factor': 0.1, u'num_layers': 152, u'num_classes': u'6', u'precision_dtype': u'float32', u'mini_batch_size': u'5', u'augmentation_type': u'crop_color_transform', u'beta_1': 0.9, u'beta_2': 0.999, u'use_pretrained_model': u'1', u'eps': 1e-08, u'weight_decay': 0.0001, u'momentum': 0, u'image_shape': u'3,224,224', u'gamma': 0.9, u'num_training_samples': u'211'}\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:06 INFO 139813596038976] Searching for .rec files in /opt/ml/input/data/train.\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:06 INFO 139813596038976] Searching for .rec files in /opt/ml/input/data/validation.\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:06 INFO 139813596038976] use_pretrained_model: 1\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:06 INFO 139813596038976] multi_label: 0\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:06 INFO 139813596038976] Using pretrained model for initializing weights and transfer learning.\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:06 INFO 139813596038976] ---- Parameters ----\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:06 INFO 139813596038976] num_layers: 152\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:06 INFO 139813596038976] data type: <type 'numpy.float32'>\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:06 INFO 139813596038976] epochs: 30\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:06 INFO 139813596038976] optimizer: sgd\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:06 INFO 139813596038976] momentum: 0.9\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:06 INFO 139813596038976] weight_decay: 0.0001\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:06 INFO 139813596038976] learning_rate: 0.001\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:06 INFO 139813596038976] num_training_samples: 211\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:06 INFO 139813596038976] mini_batch_size: 5\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:06 INFO 139813596038976] image_shape: 3,224,224\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:06 INFO 139813596038976] num_classes: 6\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:06 INFO 139813596038976] augmentation_type: crop_color_transform\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:06 INFO 139813596038976] kv_store: device\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:06 INFO 139813596038976] checkpoint_frequency not set, will store the best model\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:06 INFO 139813596038976] --------------------\u001b[0m\n",
      "\u001b[31m[20:26:06] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.1085.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...\u001b[0m\n",
      "\u001b[31m[20:26:06] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.1085.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:07 INFO 139813596038976] Setting number of threads: 7\u001b[0m\n",
      "\n",
      "2019-10-30 20:26:04 Training - Training image download completed. Training in progress.\u001b[31m[20:26:13] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.1085.0/AL2012/generic-flavor/src/src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:17 INFO 139813596038976] Epoch[0] Batch [20]#011Speed: 24.169 samples/sec#011accuracy=0.400000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:19 INFO 139813596038976] Epoch[0] Batch [40]#011Speed: 33.567 samples/sec#011accuracy=0.521951\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:19 INFO 139813596038976] Epoch[0] Train-accuracy=0.533333\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:19 INFO 139813596038976] Epoch[0] Time cost=6.048\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:20 INFO 139813596038976] Epoch[0] Validation-accuracy=0.966667\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:20 INFO 139813596038976] Storing the best model with validation accuracy: 0.966667\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:20 INFO 139813596038976] Saved checkpoint to \"/opt/ml/model/image-classification-0001.params\"\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:22 INFO 139813596038976] Epoch[1] Batch [20]#011Speed: 52.317 samples/sec#011accuracy=0.933333\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:24 INFO 139813596038976] Epoch[1] Batch [40]#011Speed: 53.272 samples/sec#011accuracy=0.868293\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:24 INFO 139813596038976] Epoch[1] Train-accuracy=0.866667\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:24 INFO 139813596038976] Epoch[1] Time cost=3.848\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:25 INFO 139813596038976] Epoch[1] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:25 INFO 139813596038976] Storing the best model with validation accuracy: 1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:25 INFO 139813596038976] Saved checkpoint to \"/opt/ml/model/image-classification-0002.params\"\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:27 INFO 139813596038976] Epoch[2] Batch [20]#011Speed: 54.663 samples/sec#011accuracy=0.971429\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:29 INFO 139813596038976] Epoch[2] Batch [40]#011Speed: 55.000 samples/sec#011accuracy=0.941463\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:29 INFO 139813596038976] Epoch[2] Train-accuracy=0.942857\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:29 INFO 139813596038976] Epoch[2] Time cost=3.724\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:30 INFO 139813596038976] Epoch[2] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:32 INFO 139813596038976] Epoch[3] Batch [20]#011Speed: 54.620 samples/sec#011accuracy=0.971429\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:34 INFO 139813596038976] Epoch[3] Batch [40]#011Speed: 54.772 samples/sec#011accuracy=0.970732\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:34 INFO 139813596038976] Epoch[3] Train-accuracy=0.971429\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:34 INFO 139813596038976] Epoch[3] Time cost=3.738\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:34 INFO 139813596038976] Epoch[3] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:37 INFO 139813596038976] Epoch[4] Batch [20]#011Speed: 54.894 samples/sec#011accuracy=0.961905\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:38 INFO 139813596038976] Epoch[4] Batch [40]#011Speed: 55.384 samples/sec#011accuracy=0.936585\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:39 INFO 139813596038976] Epoch[4] Train-accuracy=0.938095\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:39 INFO 139813596038976] Epoch[4] Time cost=3.698\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:39 INFO 139813596038976] Epoch[4] Validation-accuracy=0.988889\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:42 INFO 139813596038976] Epoch[5] Batch [20]#011Speed: 47.752 samples/sec#011accuracy=0.923810\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:43 INFO 139813596038976] Epoch[5] Batch [40]#011Speed: 50.898 samples/sec#011accuracy=0.946341\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:44 INFO 139813596038976] Epoch[5] Train-accuracy=0.947619\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:44 INFO 139813596038976] Epoch[5] Time cost=4.018\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:44 INFO 139813596038976] Epoch[5] Validation-accuracy=0.988889\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:46 INFO 139813596038976] Epoch[6] Batch [20]#011Speed: 55.254 samples/sec#011accuracy=0.961905\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:48 INFO 139813596038976] Epoch[6] Batch [40]#011Speed: 54.916 samples/sec#011accuracy=0.960976\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:48 INFO 139813596038976] Epoch[6] Train-accuracy=0.957143\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:48 INFO 139813596038976] Epoch[6] Time cost=3.729\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:49 INFO 139813596038976] Epoch[6] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:51 INFO 139813596038976] Epoch[7] Batch [20]#011Speed: 55.028 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:53 INFO 139813596038976] Epoch[7] Batch [40]#011Speed: 55.024 samples/sec#011accuracy=0.990244\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:53 INFO 139813596038976] Epoch[7] Train-accuracy=0.990476\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:53 INFO 139813596038976] Epoch[7] Time cost=3.721\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:54 INFO 139813596038976] Epoch[7] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:56 INFO 139813596038976] Epoch[8] Batch [20]#011Speed: 55.104 samples/sec#011accuracy=0.971429\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:58 INFO 139813596038976] Epoch[8] Batch [40]#011Speed: 55.258 samples/sec#011accuracy=0.980488\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:58 INFO 139813596038976] Epoch[8] Train-accuracy=0.980952\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:58 INFO 139813596038976] Epoch[8] Time cost=3.707\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:26:58 INFO 139813596038976] Epoch[8] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:01 INFO 139813596038976] Epoch[9] Batch [20]#011Speed: 54.612 samples/sec#011accuracy=0.961905\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:02 INFO 139813596038976] Epoch[9] Batch [40]#011Speed: 55.383 samples/sec#011accuracy=0.975610\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:02 INFO 139813596038976] Epoch[9] Train-accuracy=0.976190\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:02 INFO 139813596038976] Epoch[9] Time cost=3.698\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:03 INFO 139813596038976] Epoch[9] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:05 INFO 139813596038976] Epoch[10] Batch [20]#011Speed: 48.459 samples/sec#011accuracy=0.990476\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:07 INFO 139813596038976] Epoch[10] Batch [40]#011Speed: 51.463 samples/sec#011accuracy=0.995122\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:07 INFO 139813596038976] Epoch[10] Train-accuracy=0.995238\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:07 INFO 139813596038976] Epoch[10] Time cost=3.976\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:08 INFO 139813596038976] Epoch[10] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:10 INFO 139813596038976] Epoch[11] Batch [20]#011Speed: 54.920 samples/sec#011accuracy=0.971429\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:12 INFO 139813596038976] Epoch[11] Batch [40]#011Speed: 54.959 samples/sec#011accuracy=0.975610\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:12 INFO 139813596038976] Epoch[11] Train-accuracy=0.976190\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:12 INFO 139813596038976] Epoch[11] Time cost=3.725\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:13 INFO 139813596038976] Epoch[11] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:15 INFO 139813596038976] Epoch[12] Batch [20]#011Speed: 55.439 samples/sec#011accuracy=0.971429\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:17 INFO 139813596038976] Epoch[12] Batch [40]#011Speed: 55.152 samples/sec#011accuracy=0.980488\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:17 INFO 139813596038976] Epoch[12] Train-accuracy=0.980952\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:17 INFO 139813596038976] Epoch[12] Time cost=3.713\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:17 INFO 139813596038976] Epoch[12] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:20 INFO 139813596038976] Epoch[13] Batch [20]#011Speed: 54.203 samples/sec#011accuracy=0.990476\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:21 INFO 139813596038976] Epoch[13] Batch [40]#011Speed: 54.623 samples/sec#011accuracy=0.995122\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:22 INFO 139813596038976] Epoch[13] Train-accuracy=0.995238\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:22 INFO 139813596038976] Epoch[13] Time cost=3.754\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:22 INFO 139813596038976] Epoch[13] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:24 INFO 139813596038976] Epoch[14] Batch [20]#011Speed: 55.754 samples/sec#011accuracy=0.980952\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:26 INFO 139813596038976] Epoch[14] Batch [40]#011Speed: 55.915 samples/sec#011accuracy=0.985366\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:26 INFO 139813596038976] Epoch[14] Train-accuracy=0.985714\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:26 INFO 139813596038976] Epoch[14] Time cost=3.670\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:27 INFO 139813596038976] Epoch[14] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:29 INFO 139813596038976] Epoch[15] Batch [20]#011Speed: 48.786 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:31 INFO 139813596038976] Epoch[15] Batch [40]#011Speed: 51.313 samples/sec#011accuracy=0.985366\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:31 INFO 139813596038976] Epoch[15] Train-accuracy=0.985714\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:31 INFO 139813596038976] Epoch[15] Time cost=3.991\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:32 INFO 139813596038976] Epoch[15] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:34 INFO 139813596038976] Epoch[16] Batch [20]#011Speed: 55.935 samples/sec#011accuracy=0.980952\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:36 INFO 139813596038976] Epoch[16] Batch [40]#011Speed: 55.775 samples/sec#011accuracy=0.980488\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:36 INFO 139813596038976] Epoch[16] Train-accuracy=0.980952\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:36 INFO 139813596038976] Epoch[16] Time cost=3.672\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:36 INFO 139813596038976] Epoch[16] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:39 INFO 139813596038976] Epoch[17] Batch [20]#011Speed: 55.357 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:40 INFO 139813596038976] Epoch[17] Batch [40]#011Speed: 55.436 samples/sec#011accuracy=0.995122\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:41 INFO 139813596038976] Epoch[17] Train-accuracy=0.995238\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:41 INFO 139813596038976] Epoch[17] Time cost=3.694\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:41 INFO 139813596038976] Epoch[17] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:43 INFO 139813596038976] Epoch[18] Batch [20]#011Speed: 54.818 samples/sec#011accuracy=0.990476\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:45 INFO 139813596038976] Epoch[18] Batch [40]#011Speed: 54.294 samples/sec#011accuracy=0.995122\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:45 INFO 139813596038976] Epoch[18] Train-accuracy=0.995238\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:45 INFO 139813596038976] Epoch[18] Time cost=3.772\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:46 INFO 139813596038976] Epoch[18] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:48 INFO 139813596038976] Epoch[19] Batch [20]#011Speed: 55.590 samples/sec#011accuracy=0.952381\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:50 INFO 139813596038976] Epoch[19] Batch [40]#011Speed: 56.034 samples/sec#011accuracy=0.960976\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:50 INFO 139813596038976] Epoch[19] Train-accuracy=0.961905\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:50 INFO 139813596038976] Epoch[19] Time cost=3.661\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:51 INFO 139813596038976] Epoch[19] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:53 INFO 139813596038976] Epoch[20] Batch [20]#011Speed: 48.647 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:55 INFO 139813596038976] Epoch[20] Batch [40]#011Speed: 51.864 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:55 INFO 139813596038976] Epoch[20] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:55 INFO 139813596038976] Epoch[20] Time cost=3.945\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:55 INFO 139813596038976] Epoch[20] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:27:58 INFO 139813596038976] Epoch[21] Batch [20]#011Speed: 54.738 samples/sec#011accuracy=0.990476\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:00 INFO 139813596038976] Epoch[21] Batch [40]#011Speed: 54.699 samples/sec#011accuracy=0.995122\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:00 INFO 139813596038976] Epoch[21] Train-accuracy=0.995238\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:00 INFO 139813596038976] Epoch[21] Time cost=3.744\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:00 INFO 139813596038976] Epoch[21] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:02 INFO 139813596038976] Epoch[22] Batch [20]#011Speed: 54.951 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:04 INFO 139813596038976] Epoch[22] Batch [40]#011Speed: 54.878 samples/sec#011accuracy=0.990244\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:04 INFO 139813596038976] Epoch[22] Train-accuracy=0.990476\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:04 INFO 139813596038976] Epoch[22] Time cost=3.732\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:05 INFO 139813596038976] Epoch[22] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:07 INFO 139813596038976] Epoch[23] Batch [20]#011Speed: 55.158 samples/sec#011accuracy=0.990476\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:09 INFO 139813596038976] Epoch[23] Batch [40]#011Speed: 54.942 samples/sec#011accuracy=0.990244\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:09 INFO 139813596038976] Epoch[23] Train-accuracy=0.990476\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:09 INFO 139813596038976] Epoch[23] Time cost=3.727\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:10 INFO 139813596038976] Epoch[23] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:12 INFO 139813596038976] Epoch[24] Batch [20]#011Speed: 55.673 samples/sec#011accuracy=0.980952\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:14 INFO 139813596038976] Epoch[24] Batch [40]#011Speed: 55.336 samples/sec#011accuracy=0.990244\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:14 INFO 139813596038976] Epoch[24] Train-accuracy=0.990476\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:14 INFO 139813596038976] Epoch[24] Time cost=3.702\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:14 INFO 139813596038976] Epoch[24] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:17 INFO 139813596038976] Epoch[25] Batch [20]#011Speed: 49.703 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:19 INFO 139813596038976] Epoch[25] Batch [40]#011Speed: 51.939 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:19 INFO 139813596038976] Epoch[25] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:19 INFO 139813596038976] Epoch[25] Time cost=3.942\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:19 INFO 139813596038976] Epoch[25] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:22 INFO 139813596038976] Epoch[26] Batch [20]#011Speed: 54.726 samples/sec#011accuracy=0.990476\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:23 INFO 139813596038976] Epoch[26] Batch [40]#011Speed: 55.172 samples/sec#011accuracy=0.995122\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:23 INFO 139813596038976] Epoch[26] Train-accuracy=0.995238\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:23 INFO 139813596038976] Epoch[26] Time cost=3.720\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:24 INFO 139813596038976] Epoch[26] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:26 INFO 139813596038976] Epoch[27] Batch [20]#011Speed: 54.690 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:28 INFO 139813596038976] Epoch[27] Batch [40]#011Speed: 54.455 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:28 INFO 139813596038976] Epoch[27] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:28 INFO 139813596038976] Epoch[27] Time cost=3.760\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:29 INFO 139813596038976] Epoch[27] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:31 INFO 139813596038976] Epoch[28] Batch [20]#011Speed: 55.083 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:33 INFO 139813596038976] Epoch[28] Batch [40]#011Speed: 55.381 samples/sec#011accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:33 INFO 139813596038976] Epoch[28] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:33 INFO 139813596038976] Epoch[28] Time cost=3.702\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:33 INFO 139813596038976] Epoch[28] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:36 INFO 139813596038976] Epoch[29] Batch [20]#011Speed: 54.669 samples/sec#011accuracy=0.990476\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:38 INFO 139813596038976] Epoch[29] Batch [40]#011Speed: 55.651 samples/sec#011accuracy=0.990244\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:38 INFO 139813596038976] Epoch[29] Train-accuracy=0.990476\u001b[0m\n",
      "\u001b[31m[10/30/2019 20:28:38 INFO 139813596038976] Epoch[29] Time cost=3.679\u001b[0m\n",
      "\n",
      "2019-10-30 20:28:41 Uploading - Uploading generated training model\u001b[31m[10/30/2019 20:28:38 INFO 139813596038976] Epoch[29] Validation-accuracy=1.000000\u001b[0m\n",
      "\n",
      "2019-10-30 20:29:13 Completed - Training job completed\n",
      "Training seconds: 260\n",
      "Billable seconds: 260\n",
      "\n",
      "\n",
      " Finished training! The model is available for download at: s3://sagemaker-us-east-1-023375022819/recycle-images/output/IC-recycle-images-1572466969/output/model.tar.gz\n",
      "CPU times: user 831 ms, sys: 39.9 ms, total: 871 ms\n",
      "Wall time: 6min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "now = str(int(time.time()))\n",
    "training_job_name = 'IC-' + dataset_name.replace('_', '-') + '-' + now\n",
    "\n",
    "image_classifier.fit(inputs=data_channels, job_name=training_job_name, logs=True)\n",
    "\n",
    "job = image_classifier.latest_training_job\n",
    "model_path = f\"{base_dir}/{job.name}\"\n",
    "\n",
    "print(f\"\\n\\n Finished training! The model is available for download at: {image_classifier.output_path}/{job.name}/output/model.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the trained model\n",
    "Once a model has been trained, we can use the same `image_classifier` object to create a deployed, fully-managed endpoint.}\n",
    "\n",
    "More info here: https://sagemaker.readthedocs.io/en/stable/estimators.html#sagemaker.estimator.Estimator.deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: IC-recycle-images-1572466969\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the CreateEndpointConfig operation: Cannot create already existing endpoint configuration \"arn:aws:sagemaker:us-east-1:023375022819:endpoint-config/ic-recycle-images-1572466969\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, accelerator_type, endpoint_name, use_compiled_model, update_endpoint, wait, model_name, kms_key, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m             \u001b[0mkms_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkms_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         )\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/model.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, accelerator_type, endpoint_name, update_endpoint, tags, kms_key, wait)\u001b[0m\n\u001b[1;32m    459\u001b[0m                 \u001b[0maccelerator_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccelerator_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m                 \u001b[0mkms_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkms_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m             )\n\u001b[1;32m    463\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint_config_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mcreate_endpoint_config\u001b[0;34m(self, name, model_name, initial_instance_count, instance_type, accelerator_type, tags, kms_key)\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"KmsKeyId\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkms_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    659\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the CreateEndpointConfig operation: Cannot create already existing endpoint configuration \"arn:aws:sagemaker:us-east-1:023375022819:endpoint-config/ic-recycle-images-1572466969\"."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Deploying a model to an endpoint takes a few minutes to complete\n",
    "\n",
    "deployed_endpoint = image_classifier.deploy(\n",
    "    initial_instance_count = 1,\n",
    "    instance_type = 'ml.t2.medium',\n",
    "    endpoint_name = 'recycle-guru-endpoint',\n",
    "    update_endpoint = 'True'\n",
    ")\n",
    "\n",
    "# update_endpoint = 'True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up\n",
    "\n",
    "When we're done with the endpoint, we can just delete it and the backing instances will be released.  Run the following cell to delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deployed_endpoint.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling a deployed endpoint from Python code\n",
    "\n",
    "If you want to try using a deployed endpoint from Python, here's a function that you can use. It takes in a path to the image you'd like to classify, and a list of all the classes used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def classify_deployed(file_name, classes):\n",
    "    payload = None\n",
    "    with open(file_name, 'rb') as f:\n",
    "        payload = f.read()\n",
    "        payload = bytearray(payload)\n",
    "\n",
    "    deployed_endpoint.content_type = 'application/x-image'\n",
    "    result = json.loads(deployed_endpoint.predict(payload))\n",
    "    print(result)\n",
    "    best_prob_index = np.argmax(result)\n",
    "    print(best_prob_index)\n",
    "    return (classes[best_prob_index], result[best_prob_index])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name = \"s3://vision-recycle/pryan-test/water_bottle/water_bottle_0KxQl.png\"\n",
    "#file_name = 'plastic-empty-water-bottle-500x500.jpg'\n",
    "#file_name = 'iittala-arabia-24h-mug-9.jpg'\n",
    "file_name = 'coffee_2tRdu_n.png'\n",
    "classes = ['coffee','water_bottle']\n",
    "classify_deployed(file_name, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Perform Hyperparameter Tuning\n",
    "\n",
    "Often, you might not know which values for hyperparameters like `learning_rate` and `mini_batch_size` will yield acceptible results. Traditionally, this meant manually running many training jobs with different hyperparameter values, looking at each trained model's performance, and then picking a winner. \n",
    "\n",
    "This type of manual tuning is _very_ time consuming, so you can automate this process using automatic model tuning with SageMaker. Here's some example code to illustrate how to start one of these jobs using the SageMaker Python SDK.\n",
    "\n",
    "More info here about automatic model tuning: https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html\n",
    "\n",
    "More info about model tuning for the Image Classification algorithm: https://docs.aws.amazon.com/sagemaker/latest/dg/IC-tuning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import HyperparameterTuner, IntegerParameter, CategoricalParameter, ContinuousParameter\n",
    "hyperparameter_ranges = {'optimizer': CategoricalParameter(['sgd', 'adam']),\n",
    "                         'learning_rate': ContinuousParameter(0.0001, 0.1),\n",
    "                         'mini_batch_size': IntegerParameter(2, 32),\n",
    "                        }\n",
    "\n",
    "objective_metric_name = 'validation:accuracy'\n",
    "\n",
    "tuner = HyperparameterTuner(image_classifier,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            max_jobs=50,\n",
    "                            max_parallel_jobs=3)\n",
    "\n",
    "tuner.fit(inputs=data_channels, logs=True, include_cls_metadata=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great resources to continue your Deep Learning journey\n",
    "\n",
    "[3Blue1Brown’s YouTube series on Neural Networks ~ 60 Minutes](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)\n",
    "\n",
    "[Fast.ai’s Practical Deep Learning for Coders ~ 14 Hours](http://www.fast.ai/)\n",
    "    \n",
    "[Amazon's Machine Learning University ~ More than 45 hours of courses, videos, and labs](https://aws.amazon.com/training/learning-paths/machine-learning/)\n",
    "    \n",
    "[Neural Networks and Deep Learning, by Michael Neilsen ~ 6 Chapter Book](http://neuralnetworksanddeeplearning.com/)\n",
    "\n",
    "[Amazon SageMaker - Fully-managed Platform](https://aws.amazon.com/sagemaker/)\n",
    "    \n",
    "[@gabehollombe's](https://twitter.com/gabehollombe) deep learning tools and demos\n",
    "- [Jupyter Notebooks](https://github.com/gabehollombe-aws/jupyter-notebooks)\n",
    "- [Webcam S3 Uploader Tool](https://github.com/gabehollombe-aws/webcam-s3-uploader)\n",
    "- [SageMaker Inference Web Tool](https://github.com/gabehollombe-aws/webcam-sagemaker-inference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
